# PRODIGY_GA_01
Train a model to generate coherent and contextually relevant text
# Task 01 - Text Generation using GPT-2 🚀

This repository contains my solution for **Task 01** of the Generative AI Internship at **Prodigy Infotech**. The goal was to fine-tune a GPT-2 model using a custom dataset to generate contextually relevant text.

---

## 🔍 Objective

Fine-tune OpenAI’s GPT-2 model on a small, domain-specific dataset to generate text that mimics the dataset’s style.

---

## 🧰 Tools Used

- Python  
- Hugging Face Transformers  
- Hugging Face Datasets  
- Google Colab  
- GPT-2 Pretrained Model  

---

## 📚 Dataset

A simple text file (`custom_dataset.txt`) containing a mix of poetic, narrative, and technology-related lines.

Example entries:
- "Once upon a time in a faraway kingdom..."
- "Poetry is the soul's whisper..."
- "In the world of technology, artificial intelligence..."

---

## 🔁 Steps Performed

1. Installed required libraries  
2. Loaded GPT-2 model and tokenizer  
3. Tokenized the custom dataset  
4. Fine-tuned GPT-2 for 3 epochs  
5. Generated text based on a prompt  
6. Saved the fine-tuned model  

---

## 💡 Sample Output

**Prompt:** `Once upon a time`  
**Generated Text:** `Once upon a time in a land of dreams, where stars whispered secrets and the moon kept watch, a gentle breeze carried the songs of old.`

---

## 📄 Files Included

- `custom_dataset.txt` – training dataset  
- `Text_Generation_Task_Report.pdf` – detailed report  
- `GPT2_Text_Generation_Task.ipynb` – Google Colab notebook  

---

## 👤 Submitted By

**Divya Shirke**  
Intern @ Prodigy Infotech  
Subject: Generative AI  
July 2025

---

## 📌 Links

- [Colab Notebook](https://colab.research.google.com/drive/1D-8d_eYwDojHtXYDgwlzch6xT8hwMXXC)
  
