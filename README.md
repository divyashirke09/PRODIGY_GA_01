# PRODIGY_GA_01
Train a model to generate coherent and contextually relevant text
# Task 01 - Text Generation using GPT-2 ğŸš€

This repository contains my solution for **Task 01** of the Generative AI Internship at **Prodigy Infotech**. The goal was to fine-tune a GPT-2 model using a custom dataset to generate contextually relevant text.

---

## ğŸ” Objective

Fine-tune OpenAIâ€™s GPT-2 model on a small, domain-specific dataset to generate text that mimics the datasetâ€™s style.

---

## ğŸ§° Tools Used

- Python  
- Hugging Face Transformers  
- Hugging Face Datasets  
- Google Colab  
- GPT-2 Pretrained Model  

---

## ğŸ“š Dataset

A simple text file (`custom_dataset.txt`) containing a mix of poetic, narrative, and technology-related lines.

Example entries:
- "Once upon a time in a faraway kingdom..."
- "Poetry is the soul's whisper..."
- "In the world of technology, artificial intelligence..."

---

## ğŸ” Steps Performed

1. Installed required libraries  
2. Loaded GPT-2 model and tokenizer  
3. Tokenized the custom dataset  
4. Fine-tuned GPT-2 for 3 epochs  
5. Generated text based on a prompt  
6. Saved the fine-tuned model  

---

## ğŸ’¡ Sample Output

**Prompt:** `Once upon a time`  
**Generated Text:** `Once upon a time in a land of dreams, where stars whispered secrets and the moon kept watch, a gentle breeze carried the songs of old.`

---

## ğŸ“„ Files Included

- `custom_dataset.txt` â€“ training dataset  
- `Text_Generation_Task_Report.pdf` â€“ detailed report  
- `GPT2_Text_Generation_Task.ipynb` â€“ Google Colab notebook  

---

## ğŸ‘¤ Submitted By

**Divya Shirke**  
Intern @ Prodigy Infotech  
Subject: Generative AI  
July 2025

---

## ğŸ“Œ Links

- [Colab Notebook](https://colab.research.google.com/drive/1D-8d_eYwDojHtXYDgwlzch6xT8hwMXXC)
  
